## Natural language processing, word2vec + subwords, NER, neural machine translation, attention

### Learning Goals

- Understand algorithms for generating language embeddings
- Compare performance of word and subword embeddings
- Understand NLP feature engineering techniques and how they complement deep learning
- Understand tradeoffs to a variety of attentional architectures

### Exercises

- cs20si 1-3: word2vec
- cs224d: 1-3: word2vec
- cs224d: 1-4: Sentiment Analysis
- cs224d: 2-2: TensorFlow NER Window Model
- cs224d: 2-3: TensorFlow RNN Language Model
- cs224d: 3-1: Recursive Neural Network
- fast.ai: 12: Neural Machine Translation by Jointly Learning to Align and Translate
- fast.ai: 13: Neural Machine Translation of Rare Words with Subword Units
- cs20si 3: A TensorFlow chatbot
