## Natural language processing, word2vec + subwords, NER, neural machine translation, attention

### Learning Goals

- Understand state-of-the-art algorithms for generating language embeddings
- Basic familiarity with old-school NLP feature engineering techniques
- Understand tradeoffs to a variety of attentional architectures
- Understand common long-term dependency moduules: GRUs & LSTMs
- Experiment with impact of initialization on deep RNN architectures

### Exercises

- cs20si 3: A TensorFlow chatbot
- fast.ai: 13: Neural Machine Translation of Rare Words with Subword Units
- fast.ai: 12: Neural Machine Translation by Jointly Learning to Align and Translate
- cs224d: 3-1: Recursive Neural Network
- cs224d: 2-3: TensorFlow RNN Language Model
- cs224d: 2-2: TensorFlow NER Window Model
- cs224d: 1-4: Sentiment Analysis
- cs224d: 1-3: word2vec
- cs20si 1-3: word2vec
