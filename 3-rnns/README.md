## Natural language processing, word2vec + subwords, NER, neural machine translation, attention

### Learning Goals

- Understand algorithms for generating language embeddings
- Compare performance of word and subword embeddings
- Understand NLP feature engineering techniques and how they complement deep learning
- Understand tradeoffs to a variety of attentional architectures

### Exercises

- cs20si 1-3 (50%): word2vec
- cs224d: 1-3 (40%): word2vec
- cs224d: 1-4 (20%): Sentiment Analysis
- cs224d: 2-2 (35%): TensorFlow NER Window Model
- cs224d: 2-3 (45%): TensorFlow RNN Language Model
- cs224d: 3-1 (100%): Recursive Neural Network
- fast.ai: 12 Neural Machine Translation by Jointly Learning to Align and Translate
- fast.ai: 13: Neural Machine Translation of Rare Words with Subword Units
- cs20si 3: A TensorFlow chatbot
